{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# MLOps workshop with Amazon SageMaker\n",
    "\n",
    "## Module 02: Transform the data and train a model inside a Jupyter notebook.\n",
    "\n",
    "In this workshop we will demonstrate a journey to cloud-native machine learning starting from a more traditional approach to model development and training directly in Jupyter notebooks to remote managed data transformations and training with Amazon SageMaker to fully automated pipelines with SageMkaer Pipelines.\n",
    "\n",
    "In this first notebook we will predict house prices based on the well-known [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) with a simple regression model in Tensorflow 2. This public dataset contains 13 features regarding housing stock of towns in the Boston area.  Features include average number of rooms, accessibility to radial highways, adjacency to a major river, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data.  We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output.  The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "According to The [State of Data Science 2020](https://www.anaconda.com/state-of-data-science-2020) survey, data management, exploratory data analysis (EDA), feature selection, and feature engineering accounts for more than 66% of a data scientistâ€™s time.\n",
    "\n",
    "Exploratory Data Analysis is an approach in analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.\n",
    "EDA assists Data science professionals in various ways:-\n",
    "\n",
    "- Getting a better understanding of data.\n",
    "- Identifying various data patterns.\n",
    "- Getting a better understanding of the problem statement.\n",
    "\n",
    "Numerical EDA gives you some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. \n",
    "Visual EDA on the other hand will give you insight into features and target relationship and distribution.\n",
    "\n",
    "First we'll load the Boston Housing dataset and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['CRIM',\n",
    "         'ZN',\n",
    "         'INDUS',\n",
    "         'CHAS',\n",
    "         'NOX',\n",
    "         'RM',\n",
    "         'AGE',\n",
    "         'DIS',\n",
    "         'RAD',\n",
    "         'TAX',\n",
    "         'PTRATIO',\n",
    "         'B',\n",
    "         'LSTAT']\n",
    "\n",
    "df = pd.DataFrame(x_train, columns=columns)\n",
    "df[\"MEDV\"] = y_train\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical EDA\n",
    "\n",
    "Check how big is dataset, how many and of what type features it has, and what is target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 attributes in each case of the dataset. They are:\n",
    "1. CRIM - per capita crime rate by town.\n",
    "2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS - proportion of non-retail business acres per town.\n",
    "4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise).\n",
    "5. NOX - nitric oxides concentration (parts per 10 million).\n",
    "6. RM - average number of rooms per dwelling.\n",
    "7. AGE - proportion of owner-occupied units built prior to 1940.\n",
    "8. DIS - weighted distances to five Boston employment centres.\n",
    "9. RAD - index of accessibility to radial highways.\n",
    "10. TAX - full-value property-tax rate per 10K dollars.\n",
    "11. PTRATIO - pupil-teacher ratio by town.\n",
    "12. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "13. LSTAT - % lower status of the population.\n",
    "14. MEDV - Median value of owner-occupied homes in $1000's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's summarize the data to see the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual EDA\n",
    "\n",
    "Let's try to find the crime level in relation to citizen status and median value of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['medv_bins'] = pd.cut(df.MEDV, bins=5, include_lowest=True)\n",
    "df.medv_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lstat_bins'] = pd.cut(df.LSTAT, bins=[0, 7, 17, 38], labels=['richest', 'ordinary', 'poorest'], include_lowest=True)\n",
    "df.lstat_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_viol, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "sns.boxplot(x='medv_bins', y='CRIM', data=df, ax=ax_viol)\n",
    "sns.swarmplot(x='medv_bins', y='CRIM', hue='lstat_bins', data=df, ax=ax_box)\n",
    "\n",
    "ax_viol.set(xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the as the citizens status is poorer (LSTAT) and median value of houses is lower (MEDV), crime level are indeed growing (CRIM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stop with EDA for now for the sake of time needed for the rest of the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dataset transformation <a class=\"anchor\" id=\"SageMakerProcessing\">\n",
    "\n",
    "Next, we'll transform the dataset. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances. \n",
    "\n",
    "We'll now save the raw feature data, and also save the labels for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, we'll execute the data preprocessing as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_files = glob.glob('{}/raw/*.npy'.format(data_dir))\n",
    "print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "for file in input_files:\n",
    "    raw = np.load(file)\n",
    "    # only transform feature columns\n",
    "    if 'y_' not in file:\n",
    "        transformed = scaler.transform(raw)\n",
    "    if 'train' in file:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(train_dir, 'y_train.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(train_dir, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "    else:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(test_dir, 'y_test.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TEST DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(test_dir, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#  Training <a class=\"anchor\" id=\"SageMakerHostedTraining\">\n",
    "\n",
    "Now that we've prepared a dataset, we can move on to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    hidden_1 = tf.keras.layers.Dense(13, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(6, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "batch_size = 128\n",
    "epochs = 80\n",
    "learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.save('model' + '/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model/1')\n",
    "\n",
    "x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# MLOps workshop with Amazon SageMaker\n",
    "\n",
    "## Module 04: Automate the whole dataset preparation and model training pipeline with SageMaker Pipelines.\n",
    "\n",
    "In the last module of this workshop we will create a repeatable production workflow which is typically run outside notebooks. To demonstrate automating the workflow, we'll use [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) for workflow orchestration. Purpose-built for machine learning (ML), SageMaker Pipelines helps you automate different steps of the ML workflow including data processing, model training, and batch prediction (scoring), and apply conditions such as approvals for model quality. It also includes a model registry and model lineage tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Workflow Automation with SageMaker Pipelines <a class=\"anchor\" id=\"WorkflowAutomation\">\n",
    "\n",
    "In the previous parts of this notebook, we prototyped various steps of a TensorFlow project within the notebook itself, with some steps being run on external SageMaker resources (hosted training, model tuning, hosted endpoints).  Notebooks are great for prototyping, but generally are  not used in production-ready machine learning pipelines.  \n",
    "\n",
    "A very simple pipeline in SageMaker includes processing the dataset to get it ready for training, performing the actual training, and then using the model to perform some form of inference such as batch predition (scoring). We'll use SageMaker Pipelines to automate these steps, keeping the pipeline simple for now: it easily can be extended into a far more complex pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Pipeline parameters <a class=\"anchor\" id=\"PipelineParameters\">\n",
    "\n",
    "Before we begin to create the pipeline itself, we should think about how to parameterize it.  For example, we may use different instance types for different purposes, such as CPU-based types for data processing and GPU-based or more powerful types for model training.  These are all \"knobs\" of the pipeline that we can parameterize.  Parameterizing enables custom pipeline executions and schedules without having to modify the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "raw_s3 = f\"s3://{bucket}/tf-2-workflow/data/raw\"    # sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.c5.2xlarge\")\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Processing Step <a class=\"anchor\" id=\"ProcessingStep\">\n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object similar to the one above, but now parameterized so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_files = glob.glob('{}/*.npy'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "    scaler = StandardScaler()\n",
    "    x_train = np.load(os.path.join('/opt/ml/processing/input', 'x_train.npy'))\n",
    "    scaler.fit(x_train)\n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        # only transform feature columns\n",
    "        if 'y_' not in file:\n",
    "            transformed = scaler.transform(raw)\n",
    "        if 'train' in file:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TEST DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"tf-2-workflow-process\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"TF2Process\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\", s3_data_distribution_type='ShardedByS3Key'),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"./preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training and Model Creation Steps <a class=\"anchor\" id=\"TrainingModelCreation\">\n",
    "\n",
    "The following code sets up a pipeline step for a training job. We start by specifying which SageMaker prebuilt TensorFlow 2 training container to use for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "tensorflow_version = '2.3.1'\n",
    "python_version = 'py37'\n",
    "\n",
    "image_uri_train = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version,\n",
    "                                        py_version=python_version,\n",
    "                                        instance_type=training_instance_type,\n",
    "                                        image_scope=\"training\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, we specify an `Estimator` object, and define a `TrainingStep` to insert the training job in the pipeline with inputs from the previous SageMaker Processing step. Notice that we have used the hyperparameters from the best estimator in the tuning job we ran before. AutoTuning integration with SageMaker Pipelines is still under development, when it is available, we should use it to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_path = f\"s3://{bucket}/TF2WorkflowTrain\"\n",
    "training_parameters = {'epochs': 21, 'batch_size': 247, 'learning_rate': 0.138448, 'for_pipeline': 'true'}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    image_uri=image_uri_train,\n",
    "    source_dir='code',\n",
    "    entry_point='train.py',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role,\n",
    "    base_job_name=\"tf-2-workflow-train\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=training_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TF2WorkflowTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As another step, we create a SageMaker `Model` object to wrap the model artifact, and associate it with a separate SageMaker prebuilt TensorFlow Serving inference container to potentially use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "image_uri_inference = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version,\n",
    "                                        py_version=python_version,\n",
    "                                        instance_type=batch_instance_type,\n",
    "                                        image_scope=\"inference\"\n",
    "                                       )\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs_model = CreateModelInput(\n",
    "    instance_type=batch_instance_type\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"TF2WorkflowCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Batch Scoring Step <a class=\"anchor\" id=\"BatchScoringStep\">\n",
    "    \n",
    "The final step in this pipeline is offline, batch scoring (inference/prediction).  The inputs to this step will be the model we trained earlier, and the test data.  A simple, ordinary Python script is all we need to do the actual batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%writefile batch-score.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tarfile\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    install('tensorflow==2.3.1')\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path, 'r:gz') as tar:\n",
    "        tar.extractall('./model')\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model('./model/1')\n",
    "    test_path = \"/opt/ml/processing/test/\"\n",
    "    x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/batch\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = f\"{output_dir}/score-report.txt\"\n",
    "    with open(evaluation_path, 'w') as writer:\n",
    "         writer.write(f\"Test MSE : {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In regard to the SageMaker features we could use to perform batch scoring, we have several choices, including SageMaker Processing and SageMaker Batch Transform.  We'll use SageMaker Processing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_scorer = SKLearnProcessor(\n",
    "                    framework_version=framework_version,\n",
    "                    instance_type=batch_instance_type,\n",
    "                    instance_count=batch_instance_count,\n",
    "                    base_job_name=\"tf-2-workflow-batch\",\n",
    "                    sagemaker_session=sess,\n",
    "                    role=role )\n",
    "\n",
    "step_batch = ProcessingStep(\n",
    "                    name=\"TF2WorkflowBatchScoring\",\n",
    "                    processor=batch_scorer,\n",
    "                    inputs=[\n",
    "                        ProcessingInput(\n",
    "                            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                            destination=\"/opt/ml/processing/model\"\n",
    "                        ),\n",
    "                        ProcessingInput(\n",
    "                            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                                \"test\"\n",
    "                            ].S3Output.S3Uri,\n",
    "                            destination=\"/opt/ml/processing/test\"\n",
    "                        )\n",
    "                    ],\n",
    "                    outputs=[\n",
    "                        ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\"),\n",
    "                    ],\n",
    "                    code=\"./batch-score.py\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Creating and executing the pipeline <a class=\"anchor\" id=\"CreatingExecutingPipeline\">\n",
    "\n",
    "With all of the pipeline steps now defined, we can define the pipeline itself as a `Pipeline` object comprising a series of those steps.  Parallel and conditional steps also are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"TF2Workflow\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data,\n",
    "                processing_instance_type, \n",
    "                processing_instance_count, \n",
    "                training_instance_type, \n",
    "                training_instance_count,\n",
    "                batch_instance_type,\n",
    "                batch_instance_count],\n",
    "    steps=[step_process, \n",
    "           step_train, \n",
    "           step_create_model,\n",
    "           step_batch\n",
    "          ],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can inspect the pipeline definition in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After upserting its definition, we can start the pipeline with the `Pipeline` object's `start` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can now confirm that the pipeline is executing.  In the log output below, confirm that `PipelineExecutionStatus` is `Executing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the Pipeline\n",
    "\n",
    "After the pipeline started executing, you can view the pipeline run. \n",
    "\n",
    "To view them, choose the SageMakers Components and registries button.\n",
    "On the Components and registires drop down, select Pipelines.\n",
    "\n",
    "![sageMakers_components_and_registries_button](./img/pipelines_execution_1.png)\n",
    "\n",
    "\n",
    "Click the `TF2Workflow` pipeline, and then double click on the execution.\n",
    "\n",
    "![click_the_pipeline_execution](./img/pipelines_execution_2.png)\n",
    "\n",
    "\n",
    "Now you can see the pipeline executing. Click on `TF2Process` step to see additional details.\n",
    "\n",
    "![view_pipeline_execution](./img/pipelines_execution_3.png)\n",
    "\n",
    "\n",
    "On this specific step, you'll be able to see the output, logs and infomation.\n",
    "\n",
    "![view_step_details](./img/pipelines_execution_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Typically this pipeline should take about 10 minutes to complete.  We can wait for completion by invoking `wait()`. After execution is complete, we can list the status of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "execution.wait()\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check the score report\n",
    "\n",
    "After the batch scoring job in the pipeline is complete, the batch scoring report is uploaded to S3.  For simplicity, this report simply states the test MSE, but in general reports can include as much detail as desired.  Reports such as these also can be formatted for use in conditional approval steps in SageMaker Pipelines.  For example, the pipeline could have a condition step that only allows further steps to proceed only if the MSE is lower than some threshold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "report_path = f\"{step_batch.outputs[0].destination}/score-report.txt\"\n",
    "!aws s3 cp {report_path} ./score-report.txt --quiet && cat score-report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ML Lineage Tracking <a class=\"anchor\" id=\"LineageOfPipelineArtifacts\">\n",
    "\n",
    "SageMaker ML Lineage Tracking creates and stores information about the steps of a ML workflow from data preparation to model deployment. With the tracking information you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards.\n",
    "\n",
    "Let's now check out the lineage of the model generated by the pipeline above.  The lineage table identifies the resources used in training, including the timestamped train and test data sources, and the specific version of the TensorFlow 2 container in use during the training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    if execution_step['StepName'] == 'TF2WorkflowTrain':\n",
    "        display(viz.show(pipeline_execution_step=execution_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Extensions <a class=\"anchor\" id=\"Extensions\">\n",
    "\n",
    "We've covered a lot of content in this notebook:  SageMaker Processing for data transformation, Automatic Model Tuning, and SageMaker hosted training and inference.  These are central elements for most deep learning workflows in SageMaker.  Additionally, we examined how SageMaker Pipelines helps automate deep learning workflows after completion of the prototyping phase of a project.\n",
    "\n",
    "Besides all of the SageMaker features explored above, there are many other features that may be applicable to your project.  For example, to handle common problems during deep learning model training such as vanishing or exploding gradients, **SageMaker Debugger** is useful.  To manage common problems such as data drift after a model is in production, **SageMaker Model Monitor** can be applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
